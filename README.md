# Astro + DuckDB + dbt Pipeline

[![Python](https://img.shields.io/badge/python-3.11-blue)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/airflow-2.x-orange)](https://airflow.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-1.x-red)](https://www.getdbt.com/)
[![DuckDB](https://img.shields.io/badge/duckdb-0.8-green)](https://duckdb.org/)

---

## ðŸ“Š Project Overview

This repository is an **end-to-end analytics pipeline** built on the **modern data stack**, designed using the **Jaffle Shop dataset**. It demonstrates how to:

- Orchestrate workflows with **Astro/Airflow**
- Transform data with **dbt**
- Query and store data using **DuckDB** (and Postgres for optional storage)
- Use **modular, production-ready analytics patterns**

Think of it as a **playground for modern data engineering practices**â€”but fully functional enough to be production-ready.

---

## ðŸ›  Tech Stack

| Component       | Purpose                                   |
|-----------------|-------------------------------------------|
| Astro/Airflow   | Workflow orchestration & scheduling       |
| dbt             | Data modeling & transformations           |
| DuckDB          | Lightweight analytical database           |
| Postgres        | Optional production database              |
| Docker          | Containerization for reproducibility      |

---

## âš¡ Features

- Modular **Airflow DAGs** for ETL orchestration
- dbt project structured with **staging + marts**
- Seed data included for quick experimentation
- Dockerized environment for **zero hassle setup**
- Full **tests** for DAGs and transformations

---

## ðŸ“‚ Project Structure
```
â”œâ”€â”€ dags/                 # Airflow DAGs
â”œâ”€â”€ dbt/                  # dbt project
â”‚   â””â”€â”€ jaffle_shop_duckdb
â”œâ”€â”€ include/              # DuckDB files
â”œâ”€â”€ tests/                # Unit & integration tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ airflow_settings.yaml
â””â”€â”€ README.md
```
---

## Getting Started

1. **Clone the repository**
```bash
git clone https://github.com/shahidmalik4/astro_duckdb_dbt_pipeline.git
cd astro_duckdb_dbt_pipeline
```

2. **Set up the Python environment**
```
python -m venv .env
source .env/bin/activate   # or .env\Scripts\activate on Windows
pip install -r requirements.txt
```

3. **Run Airflow**
```
astro dev start
```

